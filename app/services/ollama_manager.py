# app/services/ollama_manager.py
import subprocess
import json
import time


OLLAMA_HOST = "http://127.0.0.1:11434"
REQUIRED_MODEL = "llama3.1"


def _run_cmd(cmd):
    """Run a shell command and capture output."""
    return subprocess.run(
        cmd, shell=True, capture_output=True, text=True
    )

def ping_model() -> bool:
    """
    Send a 1-token prompt to the model to ensure it's loaded into RAM.
    Returns True if successful.
    """
    import requests
    try:
        resp = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={"model": MODEL_NAME, "prompt": "ping"},
            timeout=8,
        )
        return resp.status_code == 200
    except Exception:
        return False

def ollama_is_running() -> bool:
    """Return True if Ollama service is running."""
    proc = _run_cmd("pgrep ollama")
    return proc.returncode == 0


def start_ollama():
    """Start Ollama if not running."""
    if not ollama_is_running():
        print("‚ö† Ollama is NOT running. Attempting to start...")
        proc = _run_cmd("systemctl --user start ollama")

        # Wait briefly for it to come up
        time.sleep(2)

        if not ollama_is_running():
            raise RuntimeError("‚ùå Failed to start Ollama")
        print("‚úî Ollama started successfully.")
    else:
        print("‚úî Ollama is already running.")


def list_models() -> list[str]:
    """Return a list of installed Ollama models."""
    proc = _run_cmd("ollama list --json")
    if proc.returncode != 0:
        return []

    try:
        models = json.loads(proc.stdout)
        return [m["name"] for m in models]
    except Exception:
        return []


def ensure_model_present(model: str = REQUIRED_MODEL):
    """Pull the model if it is missing."""
    models = list_models()

    if model in models:
        print(f"‚úî Model '{model}' already installed.")
        return

    print(f"‚ö† Model '{model}' not found. Pulling now...")
    proc = _run_cmd(f"ollama pull {model}")
    if proc.returncode != 0:
        raise RuntimeError(f"‚ùå Failed to pull model '{model}'")
    print(f"‚úî Model '{model}' downloaded and installed.")


def ensure_ollama_ready():
    """Top-level initializer: run once on Flask startup."""
    start_ollama()
    ensure_model_present(REQUIRED_MODEL)

def warm_model():
    """
    Ensure Ollama is running AND preload the model into RAM.
    """
    if ensure_ollama_running():
        print("üî• Warming Ollama model...")
        if ping_model():
            print("‚úî Model is warmed and ready.")
        else:
            print("‚ö† Model ping failed ‚Äî may still be cold.")

